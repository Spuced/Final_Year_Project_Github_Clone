{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **This Notebook Performs a Parameters Search on Every Possible Cleaning and Model Parameter.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test different spectral cleaning parameters by building a model pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_validate, GroupKFold\n",
    "from Spectra_Preparation_Functions import *\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the spectral data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\"../../data/exosomes.raw_spectrum_1.csv\")\n",
    "# df = pd.read_csv(\"../../data/raw_df_outliers_removed.csv\")\n",
    "# df = pd.read_csv(\"../../data/exosomes.raw_spectrum_400-1800.csv\")\n",
    "df = pd.read_csv(\"../../data/400-1800_with_raw_scaled_surface_pagerank.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelength_df = prepare_wavelength_df(df, 'Absorbance')\n",
    "wavelength_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### **Use Optuna to Find the Best Cleaning Parameter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    # Decide which cleaning steps to implement\n",
    "    pagerank_cutoff = trial.suggest_float('pagerank_cutoff', 0, 1.5)\n",
    "    despike = trial.suggest_categorical('despike', [True, False])\n",
    "    baseline_correct = trial.suggest_categorical('baseline_correct', [True, False])\n",
    "    smoothing = trial.suggest_categorical('smoothing', [True, False])\n",
    "    scaling = trial.suggest_categorical('scaling', [True, False])\n",
    "\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Filter less central spectra in each surface\n",
    "    df_copy = df_copy[df_copy['PageRank'] > pagerank_cutoff]\n",
    "\n",
    "    # Apply preprocessing based on suggested parameters\n",
    "    if despike:\n",
    "        despike_ma = trial.suggest_int('despike_ma', 5, 200)\n",
    "        despike_threshold = trial.suggest_float('despike_threshold', 3, 10, step=0.1)\n",
    "        df_copy['Absorbance'] = df_copy.groupby('SpecID')['Absorbance'].transform(lambda x: despike_group(x, ma=despike_ma, threshold = despike_threshold))\n",
    "\n",
    "    if baseline_correct:\n",
    "        lam = trial.suggest_categorical('lam', [10**2, 10**3, 10**4, 10**5, 10**6, 10**7, 10**8, 10**9, 10**10, 10**11])\n",
    "        p = trial.suggest_float('p', 0.001, 0.1, step=0.001)\n",
    "        df_copy['Baseline'] = df_copy.groupby('SpecID')['Absorbance'].transform(lambda x: asls_baseline_correction(x, lam=lam, p=p))\n",
    "        df_copy['Absorbance'] = df_copy['Absorbance'] - df_copy['Baseline']\n",
    "\n",
    "    if smoothing:\n",
    "        window_size = trial.suggest_int('window_size', 6, 251)\n",
    "        poly_order = trial.suggest_int('poly_order', 1, 5)\n",
    "        df_copy['Absorbance'] = df_copy.groupby('SpecID')['Absorbance'].transform(lambda x: savgol_filter(x, window_size, poly_order, deriv=0))\n",
    "\n",
    "    if scaling:\n",
    "        scaling_type = trial.suggest_categorical('scaling_type', ['normal', 'vector', 'snv'])\n",
    "        if scaling_type == 'normal':\n",
    "            df_copy['Absorbance'] = df_copy.groupby('SpecID')['Absorbance'].transform(lambda x: normalise(x))\n",
    "        elif scaling_type == 'vector':\n",
    "            df_copy['Absorbance'] = df_copy.groupby('SpecID')['Absorbance'].transform(lambda x: vector_normalise(x))\n",
    "        else:\n",
    "            df_copy['Absorbance'] = df_copy.groupby('SpecID')['Absorbance'].transform(lambda x: snv_normalise(x))\n",
    "\n",
    "    # Prepare data for ML\n",
    "    wavelength_df = prepare_wavelength_df(df_copy, 'Absorbance')\n",
    "    X = wavelength_df.drop(['Status', 'SurID'], axis=1)\n",
    "    y = wavelength_df['Status']\n",
    "    groups = wavelength_df['SurID']\n",
    "\n",
    "    # Suggest classifier type\n",
    "    classifier_name = trial.suggest_categorical(\"classifier\", [\"ExtraTrees\", \"RandomForest\", \"SVC\"])\n",
    "\n",
    "    if classifier_name == \"ExtraTrees\":\n",
    "        criterion = trial.suggest_categorical(\"et_criterion\", [\"gini\", \"entropy\"])\n",
    "        n_estimators = trial.suggest_int(\"et_n_estimators\", 10, 100)\n",
    "        max_depth_option = trial.suggest_categorical(\"et_max_depth_option\", [None, \"Specify\"])\n",
    "        max_depth = trial.suggest_int(\"et_max_depth\", 2, 32, log=True) if max_depth_option == \"Specify\" else None\n",
    "        class_weight = trial.suggest_categorical(\"et_class_weight_option\", [None, \"balanced\"])\n",
    "        classifier = ExtraTreesClassifier(random_state=1234, criterion=criterion, n_estimators=n_estimators, max_depth=max_depth, class_weight=class_weight)\n",
    "\n",
    "    elif classifier_name == \"RandomForest\":\n",
    "        criterion = trial.suggest_categorical(\"rf_criterion\", [\"gini\", \"entropy\"])\n",
    "        n_estimators = trial.suggest_int(\"rf_n_estimators\", 10, 100)\n",
    "        max_depth_option = trial.suggest_categorical(\"rf_max_depth_option\", [None, \"Specify\"])\n",
    "        max_depth = trial.suggest_int(\"rf_max_depth\", 2, 32, log=True) if max_depth_option == \"Specify\" else None\n",
    "        class_weight = trial.suggest_categorical(\"rf_class_weight_option\", [None, \"balanced\"])\n",
    "        classifier = RandomForestClassifier(random_state=1234, criterion=criterion, n_estimators=n_estimators, max_depth=max_depth, class_weight=class_weight)\n",
    "        \n",
    "    else:\n",
    "        classifier = SVC(\n",
    "            C=trial.suggest_float(\"svc_c\", 1e-10, 1e10, log=True),\n",
    "            kernel=trial.suggest_categorical(\"svc_kernel\", [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]),\n",
    "            gamma=trial.suggest_categorical(\"svc_gamma\", [\"scale\", \"auto\"]),\n",
    "            class_weight=trial.suggest_categorical(\"svc_class_weight_option\", [None, \"balanced\"])\n",
    "            random_state=1234,\n",
    "        )\n",
    "\n",
    "    cv = GroupKFold(n_splits=10)\n",
    "\n",
    "    # Perform cross-validation\n",
    "    scores = cross_validate(classifier, X, y, groups=groups, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "    # Return the average F1 scpre across all folds\n",
    "    return np.mean(scores['test_score'])\n",
    "\n",
    "\n",
    "sampler = optuna.samplers.TPESampler(seed=1234)  # Make the sampler behave in a deterministic way.\n",
    "study = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "\n",
    "#study = optuna.create_study(direction=\"maximize\")\n",
    "# study.optimize(objective, n_trials=100, n_jobs=-1)\n",
    "study.optimize(objective, n_trials=500, n_jobs=-1)\n",
    "\n",
    "print(study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = study.trials_dataframe(attrs=(\"number\", \"value\", \"params\", \"state\"))\n",
    "results_df.to_csv(\"../../data/studies/class_weights_acc_score_optuna_cleaning_and_model_filtered_parameters.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(study, \"../../data/studies/class_weights_acc_score_optuna_cleaning_and_model_filtered_parameters.pkl\")\n",
    "\n",
    "# loaded_study = joblib.load(\"../../data/studies/cleaning_study.pkl\")\n",
    "# print(\"Best trial until now:\")\n",
    "# print(\" Value: \", loaded_study.best_trial.value)\n",
    "# print(\" Params: \")\n",
    "# for key, value in loaded_study.best_trial.params.items():\n",
    "#     print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.visualization import plot_optimization_history, plot_slice\n",
    "\n",
    "plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_contour(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue the study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study.optimize(objective, n_trials=100, n_jobs=-1)\n",
    "\n",
    "# print(study.best_trial)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
