{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **This aims to classify the exosome staus based on a featureset derrived from the peaks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **This version performas grid search in parralel so may require better hardware.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test different spectral cleaning parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import cross_validate, GroupKFold\n",
    "from joblib import Parallel, delayed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the spectral data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\"../../data/exosomes.raw_spectrum_1.csv\")\n",
    "#df = pd.read_csv(\"../../data/exosomes.raw_spectrum_400-1800.csv\")\n",
    "df = pd.read_csv(\"../../data/400-1800_spike_removed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SpecID</th>\n",
       "      <th>Seq</th>\n",
       "      <th>WaveNumber</th>\n",
       "      <th>Absorbance</th>\n",
       "      <th>SurID</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201210-1-00</td>\n",
       "      <td>293</td>\n",
       "      <td>400.22778</td>\n",
       "      <td>1765.6628</td>\n",
       "      <td>201210-1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201210-1-00</td>\n",
       "      <td>294</td>\n",
       "      <td>400.91116</td>\n",
       "      <td>1774.7809</td>\n",
       "      <td>201210-1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201210-1-00</td>\n",
       "      <td>295</td>\n",
       "      <td>401.59454</td>\n",
       "      <td>1769.0302</td>\n",
       "      <td>201210-1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201210-1-00</td>\n",
       "      <td>296</td>\n",
       "      <td>402.27789</td>\n",
       "      <td>1756.4220</td>\n",
       "      <td>201210-1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201210-1-00</td>\n",
       "      <td>297</td>\n",
       "      <td>402.96127</td>\n",
       "      <td>1758.8690</td>\n",
       "      <td>201210-1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6239200</th>\n",
       "      <td>210526-3-09</td>\n",
       "      <td>2337</td>\n",
       "      <td>1797.03870</td>\n",
       "      <td>1617.3926</td>\n",
       "      <td>210526-3</td>\n",
       "      <td>Hyperglycemia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6239201</th>\n",
       "      <td>210526-3-09</td>\n",
       "      <td>2338</td>\n",
       "      <td>1797.72200</td>\n",
       "      <td>1633.0911</td>\n",
       "      <td>210526-3</td>\n",
       "      <td>Hyperglycemia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6239202</th>\n",
       "      <td>210526-3-09</td>\n",
       "      <td>2339</td>\n",
       "      <td>1798.40550</td>\n",
       "      <td>1633.3076</td>\n",
       "      <td>210526-3</td>\n",
       "      <td>Hyperglycemia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6239203</th>\n",
       "      <td>210526-3-09</td>\n",
       "      <td>2340</td>\n",
       "      <td>1799.08890</td>\n",
       "      <td>1641.8665</td>\n",
       "      <td>210526-3</td>\n",
       "      <td>Hyperglycemia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6239204</th>\n",
       "      <td>210526-3-09</td>\n",
       "      <td>2341</td>\n",
       "      <td>1799.77220</td>\n",
       "      <td>1618.2405</td>\n",
       "      <td>210526-3</td>\n",
       "      <td>Hyperglycemia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6239205 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              SpecID   Seq  WaveNumber  Absorbance     SurID         Status\n",
       "0        201210-1-00   293   400.22778   1765.6628  201210-1         Normal\n",
       "1        201210-1-00   294   400.91116   1774.7809  201210-1         Normal\n",
       "2        201210-1-00   295   401.59454   1769.0302  201210-1         Normal\n",
       "3        201210-1-00   296   402.27789   1756.4220  201210-1         Normal\n",
       "4        201210-1-00   297   402.96127   1758.8690  201210-1         Normal\n",
       "...              ...   ...         ...         ...       ...            ...\n",
       "6239200  210526-3-09  2337  1797.03870   1617.3926  210526-3  Hyperglycemia\n",
       "6239201  210526-3-09  2338  1797.72200   1633.0911  210526-3  Hyperglycemia\n",
       "6239202  210526-3-09  2339  1798.40550   1633.3076  210526-3  Hyperglycemia\n",
       "6239203  210526-3-09  2340  1799.08890   1641.8665  210526-3  Hyperglycemia\n",
       "6239204  210526-3-09  2341  1799.77220   1618.2405  210526-3  Hyperglycemia\n",
       "\n",
       "[6239205 rows x 6 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['201210-1-00', '201210-1-01', '201210-1-02', ..., '210526-3-07',\n",
       "       '210526-3-08', '210526-3-09'], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['SpecID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6239205 entries, 0 to 6239204\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Dtype  \n",
      "---  ------      -----  \n",
      " 0   SpecID      object \n",
      " 1   Seq         int64  \n",
      " 2   WaveNumber  float64\n",
      " 3   Absorbance  float64\n",
      " 4   SurID       object \n",
      " 5   Status      object \n",
      "dtypes: float64(2), int64(1), object(3)\n",
      "memory usage: 285.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_wavelength_df(df, absorbance_col, status_col='Status'):\n",
    "\n",
    "    # Pivot the DataFrame to get wavelengths as columns and absorbance values\n",
    "    wavelength_df = df.pivot(index='SpecID', columns='WaveNumber', values=absorbance_col).reset_index()\n",
    "    wavelength_df.columns.name = None\n",
    "\n",
    "    # Merge with the statuses based on SpecID\n",
    "    # Include the SurID to perform GroupKFold CV\n",
    "    statuses_and_surface = df[['SpecID', 'SurID', status_col]].drop_duplicates()\n",
    "    wavelength_df = pd.merge(wavelength_df, statuses_and_surface, on='SpecID')\n",
    "\n",
    "    # Set SpecID as the index\n",
    "    wavelength_df = wavelength_df.set_index('SpecID')\n",
    "\n",
    "    return wavelength_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelength_df = prepare_wavelength_df(df, 'Absorbance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>400.22778</th>\n",
       "      <th>400.91116</th>\n",
       "      <th>401.59454</th>\n",
       "      <th>402.27789</th>\n",
       "      <th>402.96127</th>\n",
       "      <th>403.64465</th>\n",
       "      <th>404.32803</th>\n",
       "      <th>405.01138</th>\n",
       "      <th>405.69476</th>\n",
       "      <th>406.37814</th>\n",
       "      <th>...</th>\n",
       "      <th>1794.9886</th>\n",
       "      <th>1795.672</th>\n",
       "      <th>1796.3553</th>\n",
       "      <th>1797.0387</th>\n",
       "      <th>1797.722</th>\n",
       "      <th>1798.4055</th>\n",
       "      <th>1799.0889</th>\n",
       "      <th>1799.7722</th>\n",
       "      <th>SurID</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SpecID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>201210-1-00</th>\n",
       "      <td>1765.6628</td>\n",
       "      <td>1774.7809</td>\n",
       "      <td>1769.0302</td>\n",
       "      <td>1756.4220</td>\n",
       "      <td>1758.8690</td>\n",
       "      <td>1763.2300</td>\n",
       "      <td>1745.2285</td>\n",
       "      <td>1773.3534</td>\n",
       "      <td>1774.7166</td>\n",
       "      <td>1753.3281</td>\n",
       "      <td>...</td>\n",
       "      <td>1210.4993</td>\n",
       "      <td>1213.9619</td>\n",
       "      <td>1225.2153</td>\n",
       "      <td>1210.0010</td>\n",
       "      <td>1210.6858</td>\n",
       "      <td>1194.4679</td>\n",
       "      <td>1195.1451</td>\n",
       "      <td>1189.8683</td>\n",
       "      <td>201210-1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201210-1-01</th>\n",
       "      <td>1966.9930</td>\n",
       "      <td>1962.4237</td>\n",
       "      <td>1954.5616</td>\n",
       "      <td>1954.3228</td>\n",
       "      <td>1963.0917</td>\n",
       "      <td>1975.0807</td>\n",
       "      <td>1979.3162</td>\n",
       "      <td>1963.4561</td>\n",
       "      <td>1968.4587</td>\n",
       "      <td>1964.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>1382.6973</td>\n",
       "      <td>1363.7004</td>\n",
       "      <td>1360.6210</td>\n",
       "      <td>1354.0477</td>\n",
       "      <td>1353.0381</td>\n",
       "      <td>1353.9978</td>\n",
       "      <td>1361.2426</td>\n",
       "      <td>1370.2874</td>\n",
       "      <td>201210-1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201210-1-02</th>\n",
       "      <td>2182.6694</td>\n",
       "      <td>2149.6565</td>\n",
       "      <td>2146.0227</td>\n",
       "      <td>2159.3459</td>\n",
       "      <td>2167.2910</td>\n",
       "      <td>2160.9861</td>\n",
       "      <td>2145.6575</td>\n",
       "      <td>2134.2004</td>\n",
       "      <td>2142.8303</td>\n",
       "      <td>2138.6309</td>\n",
       "      <td>...</td>\n",
       "      <td>1976.2070</td>\n",
       "      <td>1989.0183</td>\n",
       "      <td>1996.2838</td>\n",
       "      <td>1979.3507</td>\n",
       "      <td>1976.2002</td>\n",
       "      <td>1994.9839</td>\n",
       "      <td>1974.2030</td>\n",
       "      <td>1971.1880</td>\n",
       "      <td>201210-1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201210-1-03</th>\n",
       "      <td>2445.0837</td>\n",
       "      <td>2430.4973</td>\n",
       "      <td>2422.7927</td>\n",
       "      <td>2434.3433</td>\n",
       "      <td>2454.9700</td>\n",
       "      <td>2462.8245</td>\n",
       "      <td>2454.7007</td>\n",
       "      <td>2467.7329</td>\n",
       "      <td>2449.5161</td>\n",
       "      <td>2421.3474</td>\n",
       "      <td>...</td>\n",
       "      <td>1992.3817</td>\n",
       "      <td>2022.6331</td>\n",
       "      <td>2001.8311</td>\n",
       "      <td>2010.0946</td>\n",
       "      <td>2006.4933</td>\n",
       "      <td>2017.2891</td>\n",
       "      <td>2038.1699</td>\n",
       "      <td>2000.6475</td>\n",
       "      <td>201210-1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201210-1-04</th>\n",
       "      <td>2250.4536</td>\n",
       "      <td>2248.6235</td>\n",
       "      <td>2245.0984</td>\n",
       "      <td>2242.7173</td>\n",
       "      <td>2235.2803</td>\n",
       "      <td>2228.9585</td>\n",
       "      <td>2236.0095</td>\n",
       "      <td>2229.6091</td>\n",
       "      <td>2225.9231</td>\n",
       "      <td>2211.0359</td>\n",
       "      <td>...</td>\n",
       "      <td>2009.0385</td>\n",
       "      <td>1953.3303</td>\n",
       "      <td>1963.5698</td>\n",
       "      <td>1964.5299</td>\n",
       "      <td>1969.5634</td>\n",
       "      <td>1986.6266</td>\n",
       "      <td>1970.1484</td>\n",
       "      <td>2007.0848</td>\n",
       "      <td>201210-1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2051 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             400.22778  400.91116  401.59454  402.27789  402.96127  403.64465  \\\n",
       "SpecID                                                                          \n",
       "201210-1-00  1765.6628  1774.7809  1769.0302  1756.4220  1758.8690  1763.2300   \n",
       "201210-1-01  1966.9930  1962.4237  1954.5616  1954.3228  1963.0917  1975.0807   \n",
       "201210-1-02  2182.6694  2149.6565  2146.0227  2159.3459  2167.2910  2160.9861   \n",
       "201210-1-03  2445.0837  2430.4973  2422.7927  2434.3433  2454.9700  2462.8245   \n",
       "201210-1-04  2250.4536  2248.6235  2245.0984  2242.7173  2235.2803  2228.9585   \n",
       "\n",
       "             404.32803  405.01138  405.69476  406.37814  ...  1794.9886  \\\n",
       "SpecID                                                   ...              \n",
       "201210-1-00  1745.2285  1773.3534  1774.7166  1753.3281  ...  1210.4993   \n",
       "201210-1-01  1979.3162  1963.4561  1968.4587  1964.0000  ...  1382.6973   \n",
       "201210-1-02  2145.6575  2134.2004  2142.8303  2138.6309  ...  1976.2070   \n",
       "201210-1-03  2454.7007  2467.7329  2449.5161  2421.3474  ...  1992.3817   \n",
       "201210-1-04  2236.0095  2229.6091  2225.9231  2211.0359  ...  2009.0385   \n",
       "\n",
       "              1795.672  1796.3553  1797.0387   1797.722  1798.4055  1799.0889  \\\n",
       "SpecID                                                                          \n",
       "201210-1-00  1213.9619  1225.2153  1210.0010  1210.6858  1194.4679  1195.1451   \n",
       "201210-1-01  1363.7004  1360.6210  1354.0477  1353.0381  1353.9978  1361.2426   \n",
       "201210-1-02  1989.0183  1996.2838  1979.3507  1976.2002  1994.9839  1974.2030   \n",
       "201210-1-03  2022.6331  2001.8311  2010.0946  2006.4933  2017.2891  2038.1699   \n",
       "201210-1-04  1953.3303  1963.5698  1964.5299  1969.5634  1986.6266  1970.1484   \n",
       "\n",
       "             1799.7722     SurID  Status  \n",
       "SpecID                                    \n",
       "201210-1-00  1189.8683  201210-1  Normal  \n",
       "201210-1-01  1370.2874  201210-1  Normal  \n",
       "201210-1-02  1971.1880  201210-1  Normal  \n",
       "201210-1-03  2000.6475  201210-1  Normal  \n",
       "201210-1-04  2007.0848  201210-1  Normal  \n",
       "\n",
       "[5 rows x 2051 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wavelength_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(absorbances):\n",
    "    max_value = np.max(absorbances)\n",
    "    normalised_absorbances = absorbances / max_value\n",
    "    return normalised_absorbances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try with vector scaling instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(absorbances):\n",
    "    l2_norm = np.sqrt(np.sum(absorbances**2))  # Calculate the euclidean norm\n",
    "    normalised_absorbances = absorbances / l2_norm\n",
    "    return normalised_absorbances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### **Perform Grid-Search to find the best Assymetric Least Squares Parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the Baseline Correction and Smoothing Parameters to Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing\n",
    "# lam_values = [10**7, 10**8]\n",
    "# p_values = [0.001]\n",
    "# window_size = [9, 101]\n",
    "# poly_order = [1]\n",
    "\n",
    "# First Search\n",
    "lam_values = [10**2, 10**3, 10**4, 10**5, 10**6, 10**7, 10**8, 10**9]\n",
    "p_values = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "window_size = [5, 7, 9, 19, 25, 35, 45, 51, 65, 75, 85, 95, 101, 151, 201]\n",
    "poly_order = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SpecID</th>\n",
       "      <th>Seq</th>\n",
       "      <th>WaveNumber</th>\n",
       "      <th>Absorbance</th>\n",
       "      <th>SurID</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201210-1-00</td>\n",
       "      <td>293</td>\n",
       "      <td>400.22778</td>\n",
       "      <td>1765.6628</td>\n",
       "      <td>201210-1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201210-1-00</td>\n",
       "      <td>294</td>\n",
       "      <td>400.91116</td>\n",
       "      <td>1774.7809</td>\n",
       "      <td>201210-1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201210-1-00</td>\n",
       "      <td>295</td>\n",
       "      <td>401.59454</td>\n",
       "      <td>1769.0302</td>\n",
       "      <td>201210-1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201210-1-00</td>\n",
       "      <td>296</td>\n",
       "      <td>402.27789</td>\n",
       "      <td>1756.4220</td>\n",
       "      <td>201210-1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201210-1-00</td>\n",
       "      <td>297</td>\n",
       "      <td>402.96127</td>\n",
       "      <td>1758.8690</td>\n",
       "      <td>201210-1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6239200</th>\n",
       "      <td>210526-3-09</td>\n",
       "      <td>2337</td>\n",
       "      <td>1797.03870</td>\n",
       "      <td>1617.3926</td>\n",
       "      <td>210526-3</td>\n",
       "      <td>Hyperglycemia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6239201</th>\n",
       "      <td>210526-3-09</td>\n",
       "      <td>2338</td>\n",
       "      <td>1797.72200</td>\n",
       "      <td>1633.0911</td>\n",
       "      <td>210526-3</td>\n",
       "      <td>Hyperglycemia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6239202</th>\n",
       "      <td>210526-3-09</td>\n",
       "      <td>2339</td>\n",
       "      <td>1798.40550</td>\n",
       "      <td>1633.3076</td>\n",
       "      <td>210526-3</td>\n",
       "      <td>Hyperglycemia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6239203</th>\n",
       "      <td>210526-3-09</td>\n",
       "      <td>2340</td>\n",
       "      <td>1799.08890</td>\n",
       "      <td>1641.8665</td>\n",
       "      <td>210526-3</td>\n",
       "      <td>Hyperglycemia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6239204</th>\n",
       "      <td>210526-3-09</td>\n",
       "      <td>2341</td>\n",
       "      <td>1799.77220</td>\n",
       "      <td>1618.2405</td>\n",
       "      <td>210526-3</td>\n",
       "      <td>Hyperglycemia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6239205 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              SpecID   Seq  WaveNumber  Absorbance     SurID         Status\n",
       "0        201210-1-00   293   400.22778   1765.6628  201210-1         Normal\n",
       "1        201210-1-00   294   400.91116   1774.7809  201210-1         Normal\n",
       "2        201210-1-00   295   401.59454   1769.0302  201210-1         Normal\n",
       "3        201210-1-00   296   402.27789   1756.4220  201210-1         Normal\n",
       "4        201210-1-00   297   402.96127   1758.8690  201210-1         Normal\n",
       "...              ...   ...         ...         ...       ...            ...\n",
       "6239200  210526-3-09  2337  1797.03870   1617.3926  210526-3  Hyperglycemia\n",
       "6239201  210526-3-09  2338  1797.72200   1633.0911  210526-3  Hyperglycemia\n",
       "6239202  210526-3-09  2339  1798.40550   1633.3076  210526-3  Hyperglycemia\n",
       "6239203  210526-3-09  2340  1799.08890   1641.8665  210526-3  Hyperglycemia\n",
       "6239204  210526-3-09  2341  1799.77220   1618.2405  210526-3  Hyperglycemia\n",
       "\n",
       "[6239205 rows x 6 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Seq'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pybaselines is actually much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pybaselines.whittaker import asls\n",
    "\n",
    "def asls_baseline_correction(x, lam, p):\n",
    "        corrected, _ = asls(x, lam=lam, p=p)\n",
    "        return x - corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_combination(lam, p, df):\n",
    "    df_copy = df.copy()  # Work on a copy to avoid modifying the original df in-place\n",
    "\n",
    "    df_copy['Baseline_Corrected_Absorbance'] = df_copy.groupby('SpecID')['Absorbance'].transform(lambda x: asls_baseline_correction(x, lam, p))\n",
    "    \n",
    "    # Assuming prepare_wavelength_df function processes the df_copy and returns a DataFrame ready for ML\n",
    "    baseline_corrected = prepare_wavelength_df(df_copy, 'Baseline_Corrected_Absorbance')\n",
    "    X = baseline_corrected.drop(['Status', 'SurID'], axis=1)\n",
    "    y = baseline_corrected['Status']\n",
    "    groups = baseline_corrected['SurID']\n",
    "    \n",
    "    et = ExtraTreesClassifier(random_state=1234)\n",
    "    cv = GroupKFold(n_splits=10)\n",
    "    \n",
    "    scores = cross_validate(et, X, y, groups=groups, cv=cv, scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'])\n",
    "    \n",
    "    return {\n",
    "        'lam': lam,\n",
    "        'p': p,\n",
    "        'Accuracy': np.mean(scores['test_accuracy']),\n",
    "        'Precision': np.mean(scores['test_precision_macro']),\n",
    "        'Recall': np.mean(scores['test_recall_macro']),\n",
    "        'F1': np.mean(scores['test_f1_macro']),\n",
    "        'Accuracy Std': np.std(scores['test_accuracy']),\n",
    "        'Precision Std': np.std(scores['test_precision_macro']),\n",
    "        'Recall Std': np.std(scores['test_recall_macro']),\n",
    "        'F1 Std': np.std(scores['test_f1_macro'])\n",
    "    }\n",
    "\n",
    "# Parallel execution\n",
    "results = Parallel(n_jobs=-1)(delayed(process_combination)(lam, p, df) for lam in lam_values for p in p_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: lam=10000000.0, p=0.001 with Accuracy: 0.5798\n"
     ]
    }
   ],
   "source": [
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Identify the best parameters based on the selected criteria\n",
    "best_row = results_df.loc[results_df['Accuracy'].idxmax()]\n",
    "\n",
    "print(f\"Best Parameters: lam={best_row['lam']}, p={best_row['p']} with Accuracy: {best_row['Accuracy']:.4f}\")\n",
    "\n",
    "# Save the results to csv\n",
    "results_df.sort_values('Accuracy', ascending=False).to_csv(\"Baseline_Corrected_Results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### **Try with Different Smoothing Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_combination(window, poly, df):\n",
    "    df_copy = df.copy()  # Working on a copy to avoid modifying the original df\n",
    "    df_copy['Smoothed_Absorbance'] = df_copy.groupby('SpecID')['Absorbance'].transform(lambda x: savgol_filter(x, window, poly, deriv=0))\n",
    "    smoothed_absorbance = prepare_wavelength_df(df_copy, 'Smoothed_Absorbance')\n",
    "\n",
    "    X = smoothed_absorbance.drop(['Status', 'SurID'], axis=1)\n",
    "    y = smoothed_absorbance['Status']\n",
    "    groups = smoothed_absorbance['SurID']\n",
    "    \n",
    "    et = ExtraTreesClassifier(random_state=1234)\n",
    "    cv = GroupKFold(n_splits=10)\n",
    "    \n",
    "    scores = cross_validate(et, X, y, groups=groups, cv=cv, scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'])\n",
    "    \n",
    "    return {\n",
    "        'window': window,\n",
    "        'poly': poly,\n",
    "        'Accuracy': np.mean(scores['test_accuracy']),\n",
    "        'Precision': np.mean(scores['test_precision_macro']),\n",
    "        'Recall': np.mean(scores['test_recall_macro']),\n",
    "        'F1': np.mean(scores['test_f1_macro']),\n",
    "        'Accuracy Std': np.std(scores['test_accuracy']),\n",
    "        'Precision Std': np.std(scores['test_precision_macro']),\n",
    "        'Recall Std': np.std(scores['test_recall_macro']),\n",
    "        'F1 Std': np.std(scores['test_f1_macro'])\n",
    "    }\n",
    "\n",
    "# Execute in parallel\n",
    "results = Parallel(n_jobs=-1)(delayed(process_combination)(window, poly, df) for window in window_size for poly in poly_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: window_size=9.0, poly_order=1.0 with Accuracy: 0.5331\n"
     ]
    }
   ],
   "source": [
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Identify the best parameters based on the selected criteria\n",
    "best_row = results_df.loc[results_df['Accuracy'].idxmax()]\n",
    "\n",
    "print(f\"Best Parameters: window_size={best_row['window']}, poly_order={best_row['poly']} with Accuracy: {best_row['Accuracy']:.4f}\")\n",
    "\n",
    "# Save the results to csv\n",
    "results_df.sort_values('Accuracy', ascending=False).to_csv(\"Smoothed_Absorbance_Results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### **Try with both**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_baseline_correction(lam, p, df):\n",
    "\n",
    "    # Compute the baseline correction for a given lambda and p.\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    df_copy['Baseline_Corrected_Absorbance'] = df_copy.groupby('SpecID')['Absorbance'].transform(lambda x: asls_baseline_correction(x, lam, p))\n",
    "    \n",
    "    # Returning both the corrected DataFrame and the lam, p parameters for later use\n",
    "    return lam, p, df_copy\n",
    "\n",
    "def evaluate_model(lam, p, window, poly, df_corrected):\n",
    "\n",
    "    # Apply smoothing and evaluate the model for given lam, p, window, and poly on a baseline-corrected DataFrame.\n",
    "    \n",
    "    df_smoothed = df_corrected.copy()\n",
    "    df_smoothed['Smoothed_Baseline'] = df_smoothed.groupby('SpecID')['Baseline_Corrected_Absorbance'].transform(lambda x: savgol_filter(x, window, poly, deriv=0))\n",
    "    smoothed_baseline = prepare_wavelength_df(df_smoothed, 'Smoothed_Baseline')\n",
    "\n",
    "    X = smoothed_baseline.drop(['Status', 'SurID'], axis=1)\n",
    "    y = smoothed_baseline['Status']\n",
    "    groups = smoothed_baseline['SurID']\n",
    "    \n",
    "    et = ExtraTreesClassifier(random_state=1234)\n",
    "    cv = GroupKFold(n_splits=10)\n",
    "    \n",
    "    scores = cross_validate(et, X, y, groups=groups, cv=cv, scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'])\n",
    "    \n",
    "    return {\n",
    "        'lam': lam,\n",
    "        'p': p,\n",
    "        'window': window,\n",
    "        'poly': poly,\n",
    "        'Accuracy': np.mean(scores['test_accuracy']),\n",
    "        'Precision': np.mean(scores['test_precision_macro']),\n",
    "        'Recall': np.mean(scores['test_recall_macro']),\n",
    "        'F1': np.mean(scores['test_f1_macro']),\n",
    "        'Accuracy Std': np.std(scores['test_accuracy']),\n",
    "        'Precision Std': np.std(scores['test_precision_macro']),\n",
    "        'Recall Std': np.std(scores['test_recall_macro']),\n",
    "        'F1 Std': np.std(scores['test_f1_macro'])\n",
    "    }\n",
    "\n",
    "# Step 1: Compute all unique baseline corrections in parallel\n",
    "baseline_corrections = Parallel(n_jobs=-1)(\n",
    "    delayed(compute_baseline_correction)(lam, p, df) for lam in lam_values for p in p_values\n",
    ")\n",
    "\n",
    "# Step 2: For each baseline-corrected series, perform smoothing and model evaluation in parallel\n",
    "results = []\n",
    "for lam, p, df_corrected in baseline_corrections:\n",
    "    results.extend(\n",
    "        Parallel(n_jobs=-1)(\n",
    "            delayed(evaluate_model)(lam, p, window, poly, df_corrected) \n",
    "            for window in window_size \n",
    "            for poly in poly_order\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: lam=10000000.0, p=0.001, window_size=9.0, poly_order=1.0 with Accuracy: 0.6047\n"
     ]
    }
   ],
   "source": [
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Identify the best parameters based on the selected criteria\n",
    "best_row = results_df.loc[results_df['Accuracy'].idxmax()]\n",
    "\n",
    "print(f\"Best Parameters: lam={best_row['lam']}, p={best_row['p']}, window_size={best_row['window']}, poly_order={best_row['poly']} with Accuracy: {best_row['Accuracy']:.4f}\")\n",
    "\n",
    "# Save the results to csv\n",
    "results_df.sort_values('Accuracy', ascending=False).to_csv(\"Smoothed_Baseline_Results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### **Finally Try this using again with scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_combination(lam, p, df):\n",
    "    df_copy = df.copy()  # Work on a copy to avoid modifying the original df in-place\n",
    "    df_copy['Baseline_Corrected_Absorbance'] = df_copy.groupby('SpecID')['Absorbance'].transform(lambda x: asls_baseline_correction(x, lam, p))\n",
    "    df_copy['Scaled_Baseline'] = df_copy.groupby('SpecID')['Baseline_Corrected_Absorbance'].transform(lambda x: normalise(x))\n",
    "\n",
    "    baseline_corrected = prepare_wavelength_df(df_copy, 'Scaled_Baseline')\n",
    "    X = baseline_corrected.drop(['Status', 'SurID'], axis=1)\n",
    "    y = baseline_corrected['Status']\n",
    "    groups = baseline_corrected['SurID']\n",
    "    \n",
    "    et = ExtraTreesClassifier(random_state=1234)\n",
    "    cv = GroupKFold(n_splits=10)\n",
    "    \n",
    "    scores = cross_validate(et, X, y, groups=groups, cv=cv, scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'])\n",
    "    \n",
    "    return {\n",
    "        'lam': lam,\n",
    "        'p': p,\n",
    "        'Accuracy': np.mean(scores['test_accuracy']),\n",
    "        'Precision': np.mean(scores['test_precision_macro']),\n",
    "        'Recall': np.mean(scores['test_recall_macro']),\n",
    "        'F1': np.mean(scores['test_f1_macro']),\n",
    "        'Accuracy Std': np.std(scores['test_accuracy']),\n",
    "        'Precision Std': np.std(scores['test_precision_macro']),\n",
    "        'Recall Std': np.std(scores['test_recall_macro']),\n",
    "        'F1 Std': np.std(scores['test_f1_macro'])\n",
    "    }\n",
    "\n",
    "# Parallel execution\n",
    "results = Parallel(n_jobs=-1)(delayed(process_combination)(lam, p, df) for lam in lam_values for p in p_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: lam=100000000.0, p=0.001 with Accuracy: 0.5444\n"
     ]
    }
   ],
   "source": [
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Identify the best parameters based on the selected criteria\n",
    "best_row = results_df.loc[results_df['Accuracy'].idxmax()]\n",
    "\n",
    "print(f\"Best Parameters: lam={best_row['lam']}, p={best_row['p']} with Accuracy: {best_row['Accuracy']:.4f}\")\n",
    "\n",
    "# Save the results to csv\n",
    "results_df.sort_values('Accuracy', ascending=False).to_csv(\"Scaled_Baseline_Corrected_Results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### **Try with Different Smoothing Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_combination(window, poly, df):\n",
    "    df_copy = df.copy()  # Working on a copy to avoid modifying the original df\n",
    "    df_copy['Smoothed_Absorbance'] = df_copy.groupby('SpecID')['Absorbance'].transform(lambda x: savgol_filter(x, window, poly, deriv=0))\n",
    "    df_copy['Scaled_Smooth'] = df_copy.groupby('SpecID')['Smoothed_Absorbance'].transform(lambda x: normalise(x))\n",
    "\n",
    "    smoothed_absorbance = prepare_wavelength_df(df_copy, 'Scaled_Smooth')\n",
    "\n",
    "    X = smoothed_absorbance.drop(['Status', 'SurID'], axis=1)\n",
    "    y = smoothed_absorbance['Status']\n",
    "    groups = smoothed_absorbance['SurID']\n",
    "    \n",
    "    et = ExtraTreesClassifier(random_state=1234)\n",
    "    cv = GroupKFold(n_splits=10)\n",
    "    \n",
    "    scores = cross_validate(et, X, y, groups=groups, cv=cv, scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'])\n",
    "    \n",
    "    return {\n",
    "        'window': window,\n",
    "        'poly': poly,\n",
    "        'Accuracy': np.mean(scores['test_accuracy']),\n",
    "        'Precision': np.mean(scores['test_precision_macro']),\n",
    "        'Recall': np.mean(scores['test_recall_macro']),\n",
    "        'F1': np.mean(scores['test_f1_macro']),\n",
    "        'Accuracy Std': np.std(scores['test_accuracy']),\n",
    "        'Precision Std': np.std(scores['test_precision_macro']),\n",
    "        'Recall Std': np.std(scores['test_recall_macro']),\n",
    "        'F1 Std': np.std(scores['test_f1_macro'])\n",
    "    }\n",
    "\n",
    "# Execute in parallel\n",
    "results = Parallel(n_jobs=-1)(delayed(process_combination)(window, poly, df) for window in window_size for poly in poly_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: window_size=9.0, poly_order=1.0 with Accuracy: 0.4968\n"
     ]
    }
   ],
   "source": [
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Identify the best parameters based on the selected criteria\n",
    "best_row = results_df.loc[results_df['Accuracy'].idxmax()]\n",
    "\n",
    "print(f\"Best Parameters: window_size={best_row['window']}, poly_order={best_row['poly']} with Accuracy: {best_row['Accuracy']:.4f}\")\n",
    "\n",
    "# Save the results to csv\n",
    "results_df.sort_values('Accuracy', ascending=False).to_csv(\"Scaled_Smoothed_Absorbance_Results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### **Try with both**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_baseline_correction(lam, p, df):\n",
    "\n",
    "    # Compute the baseline correction for a given lambda and p.\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    df_copy['Baseline_Corrected_Absorbance'] = df_copy.groupby('SpecID')['Absorbance'].transform(lambda x: asls_baseline_correction(x, lam, p))\n",
    "    # Returning both the corrected DataFrame and the lam, p parameters for later use\n",
    "    return lam, p, df_copy\n",
    "\n",
    "def evaluate_model(lam, p, window, poly, df_corrected):\n",
    "\n",
    "    # Apply smoothing and evaluate the model for given lam, p, window, and poly on a baseline-corrected DataFrame.\n",
    "    \n",
    "    df_smoothed = df_corrected.copy()\n",
    "    df_smoothed['Smoothed_Baseline'] = df_smoothed.groupby('SpecID')['Baseline_Corrected_Absorbance'].transform(lambda x: savgol_filter(x, window, poly, deriv=0))\n",
    "    df_smoothed['Scaled_Smooth_Baseline'] = df_smoothed.groupby('SpecID')['Smoothed_Baseline'].transform(lambda x: normalise(x))\n",
    "    \n",
    "    smoothed_baseline = prepare_wavelength_df(df_smoothed, 'Scaled_Smooth_Baseline')\n",
    "    \n",
    "    X = smoothed_baseline.drop(['Status', 'SurID'], axis=1)\n",
    "    y = smoothed_baseline['Status']\n",
    "    groups = smoothed_baseline['SurID']\n",
    "    \n",
    "    et = ExtraTreesClassifier(random_state=1234)\n",
    "    cv = GroupKFold(n_splits=10)\n",
    "    \n",
    "    scores = cross_validate(et, X, y, groups=groups, cv=cv, scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'])\n",
    "    \n",
    "    return {\n",
    "        'lam': lam,\n",
    "        'p': p,\n",
    "        'window': window,\n",
    "        'poly': poly,\n",
    "        'Accuracy': np.mean(scores['test_accuracy']),\n",
    "        'Precision': np.mean(scores['test_precision_macro']),\n",
    "        'Recall': np.mean(scores['test_recall_macro']),\n",
    "        'F1': np.mean(scores['test_f1_macro']),\n",
    "        'Accuracy Std': np.std(scores['test_accuracy']),\n",
    "        'Precision Std': np.std(scores['test_precision_macro']),\n",
    "        'Recall Std': np.std(scores['test_recall_macro']),\n",
    "        'F1 Std': np.std(scores['test_f1_macro'])\n",
    "    }\n",
    "\n",
    "# Step 1: Compute all unique baseline corrections in parallel\n",
    "baseline_corrections = Parallel(n_jobs=-1)(\n",
    "    delayed(compute_baseline_correction)(lam, p, df) for lam in lam_values for p in p_values\n",
    ")\n",
    "\n",
    "# Step 2: For each baseline-corrected series, perform smoothing and model evaluation in parallel\n",
    "results = []\n",
    "for lam, p, df_corrected in baseline_corrections:\n",
    "    results.extend(\n",
    "        Parallel(n_jobs=-1)(\n",
    "            delayed(evaluate_model)(lam, p, window, poly, df_corrected) \n",
    "            for window in window_size \n",
    "            for poly in poly_order\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: lam=100000000.0, p=0.001, window_size=9.0, poly_order=1.0 with Accuracy: 0.5565\n"
     ]
    }
   ],
   "source": [
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Identify the best parameters based on the selected criteria\n",
    "best_row = results_df.loc[results_df['Accuracy'].idxmax()]\n",
    "\n",
    "print(f\"Best Parameters: lam={best_row['lam']}, p={best_row['p']}, window_size={best_row['window']}, poly_order={best_row['poly']} with Accuracy: {best_row['Accuracy']:.4f}\")\n",
    "\n",
    "# Save the results to csv\n",
    "results_df.sort_values('Accuracy', ascending=False).to_csv(\"Scaled_Smoothed_Baseline_Results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
