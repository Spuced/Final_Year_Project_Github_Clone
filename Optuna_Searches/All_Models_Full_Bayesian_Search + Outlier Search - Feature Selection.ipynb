{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **This Notebook Performs a Parameters Search on Every Possible Cleaning and Model Parameter.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test different spectral cleaning parameters by building a model pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_validate, GroupKFold, cross_val_score\n",
    "from Spectra_Preparation_Functions import *\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "import optuna\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif, f_classif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the spectral data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/400-1800_with_raw_scaled_surface_pagerank.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The **PageRank** value is calculated, based on the Gaussian Kernal similarity of each Scaled Raw Spectra within a Surface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This aims to remove unrepresentative spectra within each surface which we think are caused by background elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelength_df = prepare_wavelength_df(df, 'Absorbance')\n",
    "wavelength_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### **Use Optuna to Find the Best Cleaning Parameter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    # Decide which cleaning steps to implement\n",
    "    pagerank_cutoff = trial.suggest_float('pagerank_cutoff', 0, 1.5)\n",
    "    despike = trial.suggest_categorical('despike', [True, False])\n",
    "    baseline_correct = trial.suggest_categorical('baseline_correct', [True, False])\n",
    "    smoothing = trial.suggest_categorical('smoothing', [True, False])\n",
    "    scaling = trial.suggest_categorical('scaling', [True, False])\n",
    "\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Filter less central spectra in each surface\n",
    "    df_copy = df_copy[df_copy['PageRank'] > pagerank_cutoff]\n",
    "\n",
    "    # Apply preprocessing based on suggested parameters\n",
    "    if despike:\n",
    "        despike_ma = trial.suggest_int('despike_ma', 5, 100)\n",
    "        despike_threshold = trial.suggest_float('despike_threshold', 3, 10, step=0.25)\n",
    "        df_copy['Absorbance'] = df_copy.groupby('SpecID')['Absorbance'].transform(lambda x: despike_group(x, ma=despike_ma, threshold = despike_threshold))\n",
    "\n",
    "    if baseline_correct:\n",
    "        lam = trial.suggest_categorical('lam', [10**2, 10**3, 10**4, 10**5, 10**6, 10**7, 10**8, 10**9])\n",
    "        p = trial.suggest_float('p', 0.001, 0.1, step=0.001)\n",
    "        df_copy['Baseline'] = df_copy.groupby('SpecID')['Absorbance'].transform(lambda x: asls_baseline_correction(x, lam=lam, p=p))\n",
    "        df_copy['Absorbance'] = df_copy['Absorbance'] - df_copy['Baseline']\n",
    "\n",
    "    if smoothing:\n",
    "        window_size = trial.suggest_int('window_size', 5, 201)\n",
    "        poly_order = trial.suggest_int('poly_order', 1, 4)\n",
    "        df_copy['Absorbance'] = df_copy.groupby('SpecID')['Absorbance'].transform(lambda x: savgol_filter(x, window_size, poly_order, deriv=0))\n",
    "\n",
    "    if scaling:\n",
    "        scaling_type = trial.suggest_categorical('scaling_type', ['normal', 'vector', 'svn'])\n",
    "        if scaling_type == 'normal':\n",
    "            df_copy['Absorbance'] = df_copy.groupby('SpecID')['Absorbance'].transform(lambda x: normalise(x))\n",
    "        elif scaling_type == 'vector':\n",
    "            df_copy['Absorbance'] = df_copy.groupby('SpecID')['Absorbance'].transform(lambda x: vector_normalise(x))\n",
    "        else:\n",
    "            df_copy['Absorbance'] = df_copy.groupby('SpecID')['Absorbance'].transform(lambda x: svn_normalise(x))\n",
    "\n",
    "    # Prepare data for ML\n",
    "    wavelength_df = prepare_wavelength_df(df_copy, 'Absorbance')\n",
    "    X = wavelength_df.drop(['Status', 'SurID'], axis=1)\n",
    "    y = wavelength_df['Status']\n",
    "    groups = wavelength_df['SurID']\n",
    "\n",
    "    # Suggest classifier type\n",
    "    classifier_name = trial.suggest_categorical(\"classifier\", [\"ExtraTrees\", \"RandomForest\", \"SVC\"])\n",
    "\n",
    "    if classifier_name == \"ExtraTrees\":\n",
    "        classifier = ExtraTreesClassifier(\n",
    "            n_estimators=trial.suggest_int(\"et_n_estimators\", 10, 100),\n",
    "            max_depth=trial.suggest_int(\"et_max_depth\", 2, 32, log=True),\n",
    "            criterion=trial.suggest_categorical(\"et_criterion\", [\"gini\", \"entropy\", \"log_loss\"]),\n",
    "            random_state=1234,\n",
    "        )\n",
    "\n",
    "    elif classifier_name == \"RandomForest\":\n",
    "        classifier = RandomForestClassifier(\n",
    "            n_estimators=trial.suggest_int(\"rf_n_estimators\", 10, 100),\n",
    "            max_depth=trial.suggest_int(\"rf_max_depth\", 2, 32, log=True),\n",
    "            criterion=trial.suggest_categorical(\"rf_criterion\", [\"gini\", \"entropy\", \"log_loss\"]),\n",
    "            random_state=1234,\n",
    "        )\n",
    "        \n",
    "    else: # SVC\n",
    "        classifier = SVC(\n",
    "            C=trial.suggest_float(\"svc_c\", 1e-10, 1e10, log=True),\n",
    "            kernel=trial.suggest_categorical(\"svc_kernel\", [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]),\n",
    "            gamma=trial.suggest_categorical(\"svc_gamma\", [\"scale\", \"auto\"]),\n",
    "            random_state=1234,\n",
    "        )\n",
    "\n",
    "    cv = GroupKFold(n_splits=10)\n",
    "\n",
    "    num_features = trial.suggest_int(\"num_features\", 1, 2049)\n",
    "\n",
    "    # Create a pipeline with feature selection and classifier\n",
    "    pipeline = Pipeline([\n",
    "        ('feature_selection', SelectKBest(mutual_info_classif, k=num_features)),\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "\n",
    "    # Perform cross-validation\n",
    "    scores = cross_validate(pipeline, X, y, groups=groups, cv=cv, scoring='accuracy')\n",
    "\n",
    "    # Return the average accuracy across all folds\n",
    "    return np.mean(scores['test_score'])\n",
    "\n",
    "# sampler = TPESampler(seed=10)  # Make the sampler behave in a deterministic way.\n",
    "# study = optuna.create_study(direction='maximise', sampler=sampler)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "# study.optimize(objective, n_trials=100, n_jobs=-1)\n",
    "study.optimize(objective, n_trials=200, n_jobs=-1)\n",
    "\n",
    "print(study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = study.trials_dataframe(attrs=(\"number\", \"value\", \"params\", \"state\"))\n",
    "results_df.to_csv(\"../../data/studies/all_models_outliers_removed_feature_selection.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(study, \"../../data/studies/all_models_outliers_removed_feature_selection.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_study = joblib.load(\"../../data/studies/cleaning_study.pkl\")\n",
    "# print(\"Best trial until now:\")\n",
    "# print(\" Value: \", loaded_study.best_trial.value)\n",
    "# print(\" Params: \")\n",
    "# for key, value in loaded_study.best_trial.params.items():\n",
    "#     print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.visualization import plot_optimization_history, plot_slice\n",
    "\n",
    "plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_contour(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue the study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.optimize(objective, n_trials=400, n_jobs=-1)\n",
    "\n",
    "print(study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = study.trials_dataframe(attrs=(\"number\", \"value\", \"params\", \"state\"))\n",
    "results_df.to_csv(\"../../data/studies/all_models_outliers_removed_feature_selection.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(study, \"../../data/studies/all_models_outliers_removed_feature_selection.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
