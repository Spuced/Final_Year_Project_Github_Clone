{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This aims to classify the exosome staus based on a featureset derrived from the peaks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test different spectral cleaning parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, cross_validate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the spectral data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/exosomes.raw_spectrum_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SpecID</th>\n",
       "      <th>Seq</th>\n",
       "      <th>WaveNumber</th>\n",
       "      <th>Absorbance</th>\n",
       "      <th>SurID</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201210-1-00</td>\n",
       "      <td>0</td>\n",
       "      <td>200.00000</td>\n",
       "      <td>2709.3699</td>\n",
       "      <td>201210-1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201210-1-00</td>\n",
       "      <td>1</td>\n",
       "      <td>200.68336</td>\n",
       "      <td>2697.1318</td>\n",
       "      <td>201210-1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201210-1-00</td>\n",
       "      <td>2</td>\n",
       "      <td>201.36674</td>\n",
       "      <td>2696.0413</td>\n",
       "      <td>201210-1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201210-1-00</td>\n",
       "      <td>3</td>\n",
       "      <td>202.05011</td>\n",
       "      <td>2678.5925</td>\n",
       "      <td>201210-1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201210-1-00</td>\n",
       "      <td>4</td>\n",
       "      <td>202.73349</td>\n",
       "      <td>2670.8928</td>\n",
       "      <td>201210-1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8023570</th>\n",
       "      <td>210526-3-09</td>\n",
       "      <td>2630</td>\n",
       "      <td>1997.26650</td>\n",
       "      <td>1321.0371</td>\n",
       "      <td>210526-3</td>\n",
       "      <td>Hyperglycemia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8023571</th>\n",
       "      <td>210526-3-09</td>\n",
       "      <td>2631</td>\n",
       "      <td>1997.94980</td>\n",
       "      <td>1316.4056</td>\n",
       "      <td>210526-3</td>\n",
       "      <td>Hyperglycemia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8023572</th>\n",
       "      <td>210526-3-09</td>\n",
       "      <td>2632</td>\n",
       "      <td>1998.63330</td>\n",
       "      <td>1311.2640</td>\n",
       "      <td>210526-3</td>\n",
       "      <td>Hyperglycemia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8023573</th>\n",
       "      <td>210526-3-09</td>\n",
       "      <td>2633</td>\n",
       "      <td>1999.31670</td>\n",
       "      <td>1318.0909</td>\n",
       "      <td>210526-3</td>\n",
       "      <td>Hyperglycemia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8023574</th>\n",
       "      <td>210526-3-09</td>\n",
       "      <td>2634</td>\n",
       "      <td>2000.00000</td>\n",
       "      <td>1300.7710</td>\n",
       "      <td>210526-3</td>\n",
       "      <td>Hyperglycemia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8023575 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              SpecID   Seq  WaveNumber  Absorbance     SurID         Status\n",
       "0        201210-1-00     0   200.00000   2709.3699  201210-1         Normal\n",
       "1        201210-1-00     1   200.68336   2697.1318  201210-1         Normal\n",
       "2        201210-1-00     2   201.36674   2696.0413  201210-1         Normal\n",
       "3        201210-1-00     3   202.05011   2678.5925  201210-1         Normal\n",
       "4        201210-1-00     4   202.73349   2670.8928  201210-1         Normal\n",
       "...              ...   ...         ...         ...       ...            ...\n",
       "8023570  210526-3-09  2630  1997.26650   1321.0371  210526-3  Hyperglycemia\n",
       "8023571  210526-3-09  2631  1997.94980   1316.4056  210526-3  Hyperglycemia\n",
       "8023572  210526-3-09  2632  1998.63330   1311.2640  210526-3  Hyperglycemia\n",
       "8023573  210526-3-09  2633  1999.31670   1318.0909  210526-3  Hyperglycemia\n",
       "8023574  210526-3-09  2634  2000.00000   1300.7710  210526-3  Hyperglycemia\n",
       "\n",
       "[8023575 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['201210-1-00', '201210-1-01', '201210-1-02', ..., '210526-3-07',\n",
       "       '210526-3-08', '210526-3-09'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['SpecID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8023575 entries, 0 to 8023574\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Dtype  \n",
      "---  ------      -----  \n",
      " 0   SpecID      object \n",
      " 1   Seq         int64  \n",
      " 2   WaveNumber  float64\n",
      " 3   Absorbance  float64\n",
      " 4   SurID       object \n",
      " 5   Status      object \n",
      "dtypes: float64(2), int64(1), object(3)\n",
      "memory usage: 367.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train an Extra Trees Classifier on the full spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_wavelength_df(df, absorbance_col, status_col='Status'):\n",
    "\n",
    "    # Pivot the DataFrame to get wavelengths as columns and absorbance values\n",
    "    wavelength_df = df.pivot(index='SpecID', columns='WaveNumber', values=absorbance_col).reset_index()\n",
    "    wavelength_df.columns.name = None\n",
    "\n",
    "    # Merge with the statuses based on SpecID\n",
    "    statuses = df[['SpecID', status_col]].drop_duplicates()\n",
    "    wavelength_df = pd.merge(wavelength_df, statuses, on='SpecID')\n",
    "\n",
    "    # Set SpecID as the index\n",
    "    wavelength_df = wavelength_df.set_index('SpecID')\n",
    "\n",
    "    return wavelength_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelength_df = prepare_wavelength_df(df, 'Absorbance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>200.0</th>\n",
       "      <th>200.68336</th>\n",
       "      <th>201.36674</th>\n",
       "      <th>202.05011</th>\n",
       "      <th>202.73349</th>\n",
       "      <th>203.41685</th>\n",
       "      <th>204.10023</th>\n",
       "      <th>204.7836</th>\n",
       "      <th>205.46696</th>\n",
       "      <th>206.15034</th>\n",
       "      <th>...</th>\n",
       "      <th>1994.5331</th>\n",
       "      <th>1995.2164</th>\n",
       "      <th>1995.8998</th>\n",
       "      <th>1996.5831</th>\n",
       "      <th>1997.2665</th>\n",
       "      <th>1997.9498</th>\n",
       "      <th>1998.6333</th>\n",
       "      <th>1999.3167</th>\n",
       "      <th>2000.0</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SpecID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>201210-1-00</th>\n",
       "      <td>2709.3699</td>\n",
       "      <td>2697.1318</td>\n",
       "      <td>2696.0413</td>\n",
       "      <td>2678.5925</td>\n",
       "      <td>2670.8928</td>\n",
       "      <td>2652.5435</td>\n",
       "      <td>2646.3245</td>\n",
       "      <td>2690.3240</td>\n",
       "      <td>2620.3228</td>\n",
       "      <td>2609.0132</td>\n",
       "      <td>...</td>\n",
       "      <td>1100.5006</td>\n",
       "      <td>1088.7416</td>\n",
       "      <td>1092.1083</td>\n",
       "      <td>1104.9304</td>\n",
       "      <td>1084.1281</td>\n",
       "      <td>1076.9363</td>\n",
       "      <td>1089.0814</td>\n",
       "      <td>1092.8083</td>\n",
       "      <td>1086.6990</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201210-1-01</th>\n",
       "      <td>2979.3169</td>\n",
       "      <td>2985.7070</td>\n",
       "      <td>2970.1677</td>\n",
       "      <td>2947.0950</td>\n",
       "      <td>2941.8743</td>\n",
       "      <td>2942.4648</td>\n",
       "      <td>2939.9595</td>\n",
       "      <td>2938.4509</td>\n",
       "      <td>2930.9204</td>\n",
       "      <td>2915.7979</td>\n",
       "      <td>...</td>\n",
       "      <td>1246.2748</td>\n",
       "      <td>1270.4456</td>\n",
       "      <td>1272.1703</td>\n",
       "      <td>1271.8768</td>\n",
       "      <td>1270.0718</td>\n",
       "      <td>1283.9667</td>\n",
       "      <td>1286.9803</td>\n",
       "      <td>1276.4037</td>\n",
       "      <td>1268.0922</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201210-1-02</th>\n",
       "      <td>3702.5627</td>\n",
       "      <td>3592.4902</td>\n",
       "      <td>3640.8423</td>\n",
       "      <td>3593.4150</td>\n",
       "      <td>3583.6560</td>\n",
       "      <td>3583.4790</td>\n",
       "      <td>3554.3279</td>\n",
       "      <td>3507.1514</td>\n",
       "      <td>3494.4998</td>\n",
       "      <td>3490.4370</td>\n",
       "      <td>...</td>\n",
       "      <td>2028.6669</td>\n",
       "      <td>2046.8510</td>\n",
       "      <td>2094.8308</td>\n",
       "      <td>2067.8396</td>\n",
       "      <td>2043.0687</td>\n",
       "      <td>2063.5925</td>\n",
       "      <td>2086.6956</td>\n",
       "      <td>2064.7766</td>\n",
       "      <td>2064.2126</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201210-1-03</th>\n",
       "      <td>8129.5938</td>\n",
       "      <td>8222.3184</td>\n",
       "      <td>8370.2803</td>\n",
       "      <td>8534.4150</td>\n",
       "      <td>8684.1543</td>\n",
       "      <td>8805.7393</td>\n",
       "      <td>8964.5283</td>\n",
       "      <td>9220.3066</td>\n",
       "      <td>9257.7461</td>\n",
       "      <td>9399.7734</td>\n",
       "      <td>...</td>\n",
       "      <td>1682.3824</td>\n",
       "      <td>1694.8450</td>\n",
       "      <td>1710.2760</td>\n",
       "      <td>1714.6768</td>\n",
       "      <td>1746.4635</td>\n",
       "      <td>1705.4204</td>\n",
       "      <td>1703.1569</td>\n",
       "      <td>1705.2943</td>\n",
       "      <td>1687.0480</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201210-1-04</th>\n",
       "      <td>3468.5203</td>\n",
       "      <td>3463.0237</td>\n",
       "      <td>3475.2666</td>\n",
       "      <td>3468.5999</td>\n",
       "      <td>3451.7124</td>\n",
       "      <td>3439.6379</td>\n",
       "      <td>3439.1538</td>\n",
       "      <td>3444.8345</td>\n",
       "      <td>3446.6240</td>\n",
       "      <td>3438.2632</td>\n",
       "      <td>...</td>\n",
       "      <td>1725.4711</td>\n",
       "      <td>1722.2786</td>\n",
       "      <td>1757.0481</td>\n",
       "      <td>1745.6029</td>\n",
       "      <td>1728.0017</td>\n",
       "      <td>1750.2548</td>\n",
       "      <td>1747.0122</td>\n",
       "      <td>1756.1727</td>\n",
       "      <td>1747.9722</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2636 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 200.0  200.68336  201.36674  202.05011  202.73349  203.41685  \\\n",
       "SpecID                                                                          \n",
       "201210-1-00  2709.3699  2697.1318  2696.0413  2678.5925  2670.8928  2652.5435   \n",
       "201210-1-01  2979.3169  2985.7070  2970.1677  2947.0950  2941.8743  2942.4648   \n",
       "201210-1-02  3702.5627  3592.4902  3640.8423  3593.4150  3583.6560  3583.4790   \n",
       "201210-1-03  8129.5938  8222.3184  8370.2803  8534.4150  8684.1543  8805.7393   \n",
       "201210-1-04  3468.5203  3463.0237  3475.2666  3468.5999  3451.7124  3439.6379   \n",
       "\n",
       "             204.10023   204.7836  205.46696  206.15034  ...  1994.5331  \\\n",
       "SpecID                                                   ...              \n",
       "201210-1-00  2646.3245  2690.3240  2620.3228  2609.0132  ...  1100.5006   \n",
       "201210-1-01  2939.9595  2938.4509  2930.9204  2915.7979  ...  1246.2748   \n",
       "201210-1-02  3554.3279  3507.1514  3494.4998  3490.4370  ...  2028.6669   \n",
       "201210-1-03  8964.5283  9220.3066  9257.7461  9399.7734  ...  1682.3824   \n",
       "201210-1-04  3439.1538  3444.8345  3446.6240  3438.2632  ...  1725.4711   \n",
       "\n",
       "             1995.2164  1995.8998  1996.5831  1997.2665  1997.9498  1998.6333  \\\n",
       "SpecID                                                                          \n",
       "201210-1-00  1088.7416  1092.1083  1104.9304  1084.1281  1076.9363  1089.0814   \n",
       "201210-1-01  1270.4456  1272.1703  1271.8768  1270.0718  1283.9667  1286.9803   \n",
       "201210-1-02  2046.8510  2094.8308  2067.8396  2043.0687  2063.5925  2086.6956   \n",
       "201210-1-03  1694.8450  1710.2760  1714.6768  1746.4635  1705.4204  1703.1569   \n",
       "201210-1-04  1722.2786  1757.0481  1745.6029  1728.0017  1750.2548  1747.0122   \n",
       "\n",
       "             1999.3167     2000.0  Status  \n",
       "SpecID                                     \n",
       "201210-1-00  1092.8083  1086.6990  Normal  \n",
       "201210-1-01  1276.4037  1268.0922  Normal  \n",
       "201210-1-02  2064.7766  2064.2126  Normal  \n",
       "201210-1-03  1705.2943  1687.0480  Normal  \n",
       "201210-1-04  1756.1727  1747.9722  Normal  \n",
       "\n",
       "[5 rows x 2636 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wavelength_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(absorbances):\n",
    "    max_value = np.max(absorbances)\n",
    "    normalized_absorbances = absorbances / max_value\n",
    "    return normalized_absorbances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Calculate the baseline using Asymmetric Least Squares, then subtract it from the spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_als_optimized(y, lam, p, niter=10):\n",
    "    L = len(y)\n",
    "    D = sparse.diags([1,-2,1],[0,-1,-2], shape=(L,L-2))\n",
    "    D = lam * D.dot(D.transpose()) # Precompute this term since it does not depend on `w`\n",
    "    w = np.ones(L)\n",
    "    W = sparse.spdiags(w, 0, L, L)\n",
    "    for i in range(niter):\n",
    "        W.setdiag(w) # Do not create a new matrix, just update diagonal values\n",
    "        Z = W + D\n",
    "        z = spsolve(Z, w*y)\n",
    "        w = p * (y > z) + (1-p) * (y < z)\n",
    "    return z\n",
    "\n",
    "# lam = 10 ** 8\n",
    "# p = 0.05\n",
    "# df['Baseline_Corrected_Absorbance'] = df.groupby('SpecID')['Despiked_Absorbance'].transform(lambda x: baseline_als_optimized(x, lam=lam, p=p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### Perform Grid-Search to find the best Assymetric Least Squares Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the Baseline Correction and Smoothing Parameters to Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lam_values = [10**4, 10**5, 10**6, 10**7]\n",
    "# p_values = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "# window_size = [5, 7, 9, 19, 25, 51, 101, 151, 251]\n",
    "# poly_order = [1, 2, 3]\n",
    "\n",
    "# lam_values = [10**7, 10**8, 10**9]\n",
    "# p_values = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "# window_size = [9, 19, 25, 51, 101, 151]\n",
    "# poly_order = [1, 2]\n",
    "\n",
    "lam_values = [10**7, 10**8]\n",
    "p_values = [0.001]\n",
    "window_size = [9, 101]\n",
    "poly_order = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lam \u001b[38;5;129;01min\u001b[39;00m lam_values:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m p_values:\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;66;03m# Apply baseline correction\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m         df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBaseline_Corrected_Absorbance\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAbsorbance\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSpecID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAbsorbance\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline_als_optimized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m         baseline_corrected \u001b[38;5;241m=\u001b[39m prepare_wavelength_df(df, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBaseline_Corrected_Absorbance\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m         X \u001b[38;5;241m=\u001b[39m baseline_corrected\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStatus\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\FX 8320\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py:446\u001b[0m, in \u001b[0;36mSeriesGroupBy.transform\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(klass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeries\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    444\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_transform_template)\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, \u001b[38;5;241m*\u001b[39margs, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\FX 8320\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1851\u001b[0m, in \u001b[0;36mGroupBy._transform\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1848\u001b[0m func \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mget_cython_func(func) \u001b[38;5;129;01mor\u001b[39;00m func\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m-> 1851\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1853\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m func \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m base\u001b[38;5;241m.\u001b[39mtransform_kernel_allowlist:\n\u001b[0;32m   1854\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not a valid function name for transform(name)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\FX 8320\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py:479\u001b[0m, in \u001b[0;36mSeriesGroupBy._transform_general\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrouper\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selected_obj, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis\n\u001b[0;32m    476\u001b[0m ):\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;66;03m# this setattr is needed for test_transform_lambda_with_datetimetz\u001b[39;00m\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(group, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, name)\n\u001b[1;32m--> 479\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    481\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(klass(res, index\u001b[38;5;241m=\u001b[39mgroup\u001b[38;5;241m.\u001b[39mindex))\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# check for empty \"results\" to avoid concat ValueError\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 6\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lam \u001b[38;5;129;01min\u001b[39;00m lam_values:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m p_values:\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;66;03m# Apply baseline correction\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m         df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBaseline_Corrected_Absorbance\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAbsorbance\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpecID\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAbsorbance\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mbaseline_als_optimized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      8\u001b[0m         baseline_corrected \u001b[38;5;241m=\u001b[39m prepare_wavelength_df(df, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBaseline_Corrected_Absorbance\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m         X \u001b[38;5;241m=\u001b[39m baseline_corrected\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStatus\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m, in \u001b[0;36mbaseline_als_optimized\u001b[1;34m(y, lam, p, niter)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbaseline_als_optimized\u001b[39m(y, lam, p, niter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m      2\u001b[0m     L \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(y)\n\u001b[1;32m----> 3\u001b[0m     D \u001b[38;5;241m=\u001b[39m \u001b[43msparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiags\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[43mL\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     D \u001b[38;5;241m=\u001b[39m lam \u001b[38;5;241m*\u001b[39m D\u001b[38;5;241m.\u001b[39mdot(D\u001b[38;5;241m.\u001b[39mtranspose()) \u001b[38;5;66;03m# Precompute this term since it does not depend on `w`\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     w \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(L)\n",
      "File \u001b[1;32mc:\\Users\\FX 8320\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\sparse\\_construct.py:176\u001b[0m, in \u001b[0;36mdiags\u001b[1;34m(diagonals, offsets, shape, format, dtype)\u001b[0m\n\u001b[0;32m    173\u001b[0m M \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m([\u001b[38;5;28mmin\u001b[39m(m \u001b[38;5;241m+\u001b[39m offset, n \u001b[38;5;241m-\u001b[39m offset) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, offset)\n\u001b[0;32m    174\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m offset \u001b[38;5;129;01min\u001b[39;00m offsets])\n\u001b[0;32m    175\u001b[0m M \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, M)\n\u001b[1;32m--> 176\u001b[0m data_arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(offsets), M), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    178\u001b[0m K \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(m, n)\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, diagonal \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(diagonals):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for lam in lam_values:\n",
    "    for p in p_values:\n",
    "        # Apply baseline correction\n",
    "        df['Baseline_Corrected_Absorbance'] = df['Absorbance'] - df.groupby('SpecID')['Absorbance'].transform(lambda x: baseline_als_optimized(x, lam=lam, p=p))\n",
    "                        \n",
    "        baseline_corrected = prepare_wavelength_df(df, 'Baseline_Corrected_Absorbance')\n",
    "        X = baseline_corrected.drop(['Status'], axis=1)\n",
    "        y = baseline_corrected['Status']\n",
    "        \n",
    "        et = ExtraTreesClassifier(random_state=1234)\n",
    "        cv = KFold(n_splits=10, shuffle=True, random_state=1234)\n",
    "        \n",
    "        # Perform cross-validation with multiple scoring metrics\n",
    "        scores = cross_validate(et, X, y, cv=cv, scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'])\n",
    "        \n",
    "        # Append the results\n",
    "        results.append({\n",
    "            'lam': lam,\n",
    "            'p': p,\n",
    "            'Accuracy': np.mean(scores['test_accuracy']),\n",
    "            'Precision': np.mean(scores['test_precision_macro']),\n",
    "            'Recall': np.mean(scores['test_recall_macro']),\n",
    "            'F1': np.mean(scores['test_f1_macro']),\n",
    "            'Accuracy Std': np.std(scores['test_accuracy']),\n",
    "            'Precision Std': np.std(scores['test_precision_macro']),\n",
    "            'Recall Std': np.std(scores['test_recall_macro']),\n",
    "            'F1 Std': np.std(scores['test_f1_macro'])\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it with Parallel Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def process_combination(lam, p, df):\n",
    "    df_copy = df.copy()  # Work on a copy to avoid modifying the original df in-place\n",
    "    df_copy['Baseline_Corrected_Absorbance'] = df_copy['Absorbance'] - df_copy.groupby('SpecID')['Absorbance'].transform(lambda x: baseline_als_optimized(x, lam=lam, p=p))\n",
    "    baseline_corrected = prepare_wavelength_df(df_copy, 'Baseline_Corrected_Absorbance')\n",
    "    X = baseline_corrected.drop(['Status'], axis=1)\n",
    "    y = baseline_corrected['Status']\n",
    "    \n",
    "    et = ExtraTreesClassifier(random_state=1234)\n",
    "    cv = KFold(n_splits=10, shuffle=True, random_state=1234)\n",
    "    \n",
    "    scores = cross_validate(et, X, y, cv=cv, scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'])\n",
    "    \n",
    "    return {\n",
    "        'lam': lam,\n",
    "        'p': p,\n",
    "        'Accuracy': np.mean(scores['test_accuracy']),\n",
    "        'Precision': np.mean(scores['test_precision_macro']),\n",
    "        'Recall': np.mean(scores['test_recall_macro']),\n",
    "        'F1': np.mean(scores['test_f1_macro']),\n",
    "        'Accuracy Std': np.std(scores['test_accuracy']),\n",
    "        'Precision Std': np.std(scores['test_precision_macro']),\n",
    "        'Recall Std': np.std(scores['test_recall_macro']),\n",
    "        'F1 Std': np.std(scores['test_f1_macro'])\n",
    "    }\n",
    "\n",
    "# Define your parameters\n",
    "lam_values = [10**7, 10**8, 10**9]\n",
    "p_values = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "\n",
    "# Parallel execution\n",
    "results = Parallel(n_jobs=-1)(delayed(process_combination)(lam, p, df) for lam in lam_values for p in p_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: lam=1000000000.0, p=0.05 with Accuracy: 0.9343\n"
     ]
    }
   ],
   "source": [
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Identify the best parameters based on the selected criteria\n",
    "best_row = results_df.loc[results_df['Accuracy'].idxmax()]\n",
    "\n",
    "print(f\"Best Parameters: lam={best_row['lam']}, p={best_row['p']} with Accuracy: {best_row['Accuracy']:.4f}\")\n",
    "\n",
    "# Save the results to csv\n",
    "results_df.sort_values('Accuracy', ascending=False).to_csv(\"Baseline_Corrected_Results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### Try with Different Smoothing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m cv \u001b[38;5;241m=\u001b[39m KFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1234\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Perform cross-validation with multiple scoring metrics\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43met\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprecision_macro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrecall_macro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mf1_macro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Append the results\u001b[39;00m\n\u001b[0;32m     19\u001b[0m results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwindow\u001b[39m\u001b[38;5;124m'\u001b[39m: window,\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpoly\u001b[39m\u001b[38;5;124m'\u001b[39m: poly,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF1 Std\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mstd(scores[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_f1_macro\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     30\u001b[0m })\n",
      "File \u001b[1;32mc:\\Users\\FX 8320\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    265\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 266\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\FX 8320\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\FX 8320\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\FX 8320\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\FX 8320\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\FX 8320\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    684\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 686\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    690\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\FX 8320\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:473\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    462\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    465\u001b[0m ]\n\u001b[0;32m    467\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 473\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\FX 8320\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\FX 8320\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\FX 8320\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\FX 8320\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\FX 8320\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:186\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    184\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39mcurr_sample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 186\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree\n",
      "File \u001b[1;32mc:\\Users\\FX 8320\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py:889\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    859\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    860\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    861\u001b[0m \n\u001b[0;32m    862\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 889\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    895\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\FX 8320\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    370\u001b[0m         splitter,\n\u001b[0;32m    371\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    377\u001b[0m     )\n\u001b[1;32m--> 379\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for window in window_size:\n",
    "    for poly in poly_order:\n",
    "        # Apply smoothing\n",
    "        df['Smoothed_Absorbance'] = df.groupby('SpecID')['Absorbance'].transform(lambda x: savgol_filter(x, window, poly, deriv=0))\n",
    "        \n",
    "        smoothed_absorbance = prepare_wavelength_df(df, 'Smoothed_Absorbance')\n",
    "        X = smoothed_absorbance.drop(['Status'], axis=1)\n",
    "        y = smoothed_absorbance['Status']\n",
    "        \n",
    "        et = ExtraTreesClassifier(random_state=1234)\n",
    "        cv = KFold(n_splits=10, shuffle=True, random_state=1234)\n",
    "        \n",
    "        # Perform cross-validation with multiple scoring metrics\n",
    "        scores = cross_validate(et, X, y, cv=cv, scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'])\n",
    "        \n",
    "        # Append the results\n",
    "        results.append({\n",
    "            'window': window,\n",
    "            'poly': poly,\n",
    "            'Accuracy': np.mean(scores['test_accuracy']),\n",
    "            'Precision': np.mean(scores['test_precision_macro']),\n",
    "            'Recall': np.mean(scores['test_recall_macro']),\n",
    "            'F1': np.mean(scores['test_f1_macro']),\n",
    "            'Accuracy Std': np.std(scores['test_accuracy']),\n",
    "            'Precision Std': np.std(scores['test_precision_macro']),\n",
    "            'Recall Std': np.std(scores['test_recall_macro']),\n",
    "            'F1 Std': np.std(scores['test_f1_macro'])\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: window_size=7.0, poly_order=2.0 with Accuracy: 0.8831\n"
     ]
    }
   ],
   "source": [
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Identify the best parameters based on the selected criteria\n",
    "best_row = results_df.loc[results_df['Accuracy'].idxmax()]\n",
    "\n",
    "print(f\"Best Parameters: window_size={best_row['window']}, poly_order={best_row['poly']} with Accuracy: {best_row['Accuracy']:.4f}\")\n",
    "\n",
    "# Save the results to csv\n",
    "results_df.sort_values('Accuracy', ascending=False).to_csv(\"Smoothed_Absorbance_Results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">##### Try with both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for lam in lam_values:\n",
    "    for p in p_values:\n",
    "        # Apply baseline correction\n",
    "        df['Baseline_Corrected_Absorbance'] = df['Absorbance'] - df.groupby('SpecID')['Absorbance'].transform(lambda x: baseline_als_optimized(x, lam=lam, p=p))\n",
    "                \n",
    "        for window in window_size:\n",
    "            for poly in poly_order:\n",
    "                # Apply smoothing\n",
    "                df['Smoothed_Baseline'] = df.groupby('SpecID')['Baseline_Corrected_Absorbance'].transform(lambda x: savgol_filter(x, window, poly, deriv=0))\n",
    "                \n",
    "                smoothed_baseline = prepare_wavelength_df(df, 'Smoothed_Baseline')\n",
    "                X = smoothed_baseline.drop(['Status'], axis=1)\n",
    "                y = smoothed_baseline['Status']\n",
    "                \n",
    "                et = ExtraTreesClassifier(random_state=1234)\n",
    "                cv = KFold(n_splits=10, shuffle=True, random_state=1234)\n",
    "                \n",
    "                # Perform cross-validation with multiple scoring metrics\n",
    "                scores = cross_validate(et, X, y, cv=cv, scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'])\n",
    "                \n",
    "                # Append the results\n",
    "                results.append({\n",
    "                    'lam': lam,\n",
    "                    'p': p,\n",
    "                    'window': window,\n",
    "                    'poly': poly,\n",
    "                    'Accuracy': np.mean(scores['test_accuracy']),\n",
    "                    'Precision': np.mean(scores['test_precision_macro']),\n",
    "                    'Recall': np.mean(scores['test_recall_macro']),\n",
    "                    'F1': np.mean(scores['test_f1_macro']),\n",
    "                    'Accuracy Std': np.std(scores['test_accuracy']),\n",
    "                    'Precision Std': np.std(scores['test_precision_macro']),\n",
    "                    'Recall Std': np.std(scores['test_recall_macro']),\n",
    "                    'F1 Std': np.std(scores['test_f1_macro'])\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: lam=10000000.0, p=0.05, window_size=51.0, poly_order=2.0 with Accuracy: 0.9524\n"
     ]
    }
   ],
   "source": [
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Identify the best parameters based on the selected criteria\n",
    "best_row = results_df.loc[results_df['Accuracy'].idxmax()]\n",
    "\n",
    "print(f\"Best Parameters: lam={best_row['lam']}, p={best_row['p']}, window_size={best_row['window']}, poly_order={best_row['poly']} with Accuracy: {best_row['Accuracy']:.4f}\")\n",
    "\n",
    "# Save the results to csv\n",
    "results_df.sort_values('Accuracy', ascending=False).to_csv(\"Smoothed_Baseline_Results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### Finally Try this using again with scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for lam in lam_values:\n",
    "    for p in p_values:\n",
    "        # Apply baseline correction\n",
    "        df['Baseline_Corrected_Absorbance'] = df['Absorbance'] - df.groupby('SpecID')['Absorbance'].transform(lambda x: baseline_als_optimized(x, lam=lam, p=p))\n",
    "        df['Scaled_Baseline'] = df.groupby('SpecID')['Baseline_Corrected_Absorbance'].transform(lambda x: normalise(x))\n",
    "                        \n",
    "        baseline_corrected = prepare_wavelength_df(df, 'Scaled_Baseline')\n",
    "        X = baseline_corrected.drop(['Status'], axis=1)\n",
    "        y = baseline_corrected['Status']\n",
    "        \n",
    "        et = ExtraTreesClassifier(random_state=1234)\n",
    "        cv = KFold(n_splits=10, shuffle=True, random_state=1234)\n",
    "        \n",
    "        # Perform cross-validation with multiple scoring metrics\n",
    "        scores = cross_validate(et, X, y, cv=cv, scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'])\n",
    "        \n",
    "        # Append the results\n",
    "        results.append({\n",
    "            'lam': lam,\n",
    "            'p': p,\n",
    "            'Accuracy': np.mean(scores['test_accuracy']),\n",
    "            'Precision': np.mean(scores['test_precision_macro']),\n",
    "            'Recall': np.mean(scores['test_recall_macro']),\n",
    "            'F1': np.mean(scores['test_f1_macro']),\n",
    "            'Accuracy Std': np.std(scores['test_accuracy']),\n",
    "            'Precision Std': np.std(scores['test_precision_macro']),\n",
    "            'Recall Std': np.std(scores['test_recall_macro']),\n",
    "            'F1 Std': np.std(scores['test_f1_macro'])\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: lam=10000000.0, p=0.001 with Accuracy: 0.8959\n"
     ]
    }
   ],
   "source": [
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Identify the best parameters based on the selected criteria\n",
    "best_row = results_df.loc[results_df['Accuracy'].idxmax()]\n",
    "\n",
    "print(f\"Best Parameters: lam={best_row['lam']}, p={best_row['p']} with Accuracy: {best_row['Accuracy']:.4f}\")\n",
    "\n",
    "# Save the results to csv\n",
    "results_df.sort_values('Accuracy', ascending=False).to_csv(\"Scaled_Baseline_Corrected_Results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### Try with Different Smoothing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for window in window_size:\n",
    "    for poly in poly_order:\n",
    "        # Apply smoothing\n",
    "        df['Smoothed_Absorbance'] = df.groupby('SpecID')['Absorbance'].transform(lambda x: savgol_filter(x, window, poly, deriv=0))\n",
    "        df['Scaled_Smooth'] = df.groupby('SpecID')['Smoothed_Absorbance'].transform(lambda x: normalise(x))\n",
    "        \n",
    "        smoothed_absorbance = prepare_wavelength_df(df, 'Scaled_Smooth')\n",
    "        X = smoothed_absorbance.drop(['Status'], axis=1)\n",
    "        y = smoothed_absorbance['Status']\n",
    "        \n",
    "        et = ExtraTreesClassifier(random_state=1234)\n",
    "        cv = KFold(n_splits=10, shuffle=True, random_state=1234)\n",
    "        \n",
    "        # Perform cross-validation with multiple scoring metrics\n",
    "        scores = cross_validate(et, X, y, cv=cv, scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'])\n",
    "        \n",
    "        # Append the results\n",
    "        results.append({\n",
    "            'window': window,\n",
    "            'poly': poly,\n",
    "            'Accuracy': np.mean(scores['test_accuracy']),\n",
    "            'Precision': np.mean(scores['test_precision_macro']),\n",
    "            'Recall': np.mean(scores['test_recall_macro']),\n",
    "            'F1': np.mean(scores['test_f1_macro']),\n",
    "            'Accuracy Std': np.std(scores['test_accuracy']),\n",
    "            'Precision Std': np.std(scores['test_precision_macro']),\n",
    "            'Recall Std': np.std(scores['test_recall_macro']),\n",
    "            'F1 Std': np.std(scores['test_f1_macro'])\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: window_size=101.0, poly_order=2.0 with Accuracy: 0.9507\n"
     ]
    }
   ],
   "source": [
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Identify the best parameters based on the selected criteria\n",
    "best_row = results_df.loc[results_df['Accuracy'].idxmax()]\n",
    "\n",
    "print(f\"Best Parameters: window_size={best_row['window']}, poly_order={best_row['poly']} with Accuracy: {best_row['Accuracy']:.4f}\")\n",
    "\n",
    "# Save the results to csv\n",
    "results_df.sort_values('Accuracy', ascending=False).to_csv(\"Scaled_Smoothed_Absorbance_Results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">##### Try with both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for lam in lam_values:\n",
    "    for p in p_values:\n",
    "        # Apply baseline correction\n",
    "        df['Baseline_Corrected_Absorbance'] = df['Absorbance'] - df.groupby('SpecID')['Absorbance'].transform(lambda x: baseline_als_optimized(x, lam=lam, p=p))\n",
    "                \n",
    "        for window in window_size:\n",
    "            for poly in poly_order:\n",
    "                # Apply smoothing\n",
    "                df['Smoothed_Baseline'] = df.groupby('SpecID')['Baseline_Corrected_Absorbance'].transform(lambda x: savgol_filter(x, window, poly, deriv=0))\n",
    "                df['Scaled_Smooth_Baseline'] = df.groupby('SpecID')['Smoothed_Baseline'].transform(lambda x: normalise(x))\n",
    "                \n",
    "                smoothed_baseline = prepare_wavelength_df(df, 'Scaled_Smooth_Baseline')\n",
    "                X = smoothed_baseline.drop(['Status'], axis=1)\n",
    "                y = smoothed_baseline['Status']\n",
    "                \n",
    "                et = ExtraTreesClassifier(random_state=1234)\n",
    "                cv = KFold(n_splits=10, shuffle=True, random_state=1234)\n",
    "                \n",
    "                # Perform cross-validation with multiple scoring metrics\n",
    "                scores = cross_validate(et, X, y, cv=cv, scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'])\n",
    "                \n",
    "                # Append the results\n",
    "                results.append({\n",
    "                    'lam': lam,\n",
    "                    'p': p,\n",
    "                    'window': window,\n",
    "                    'poly': poly,\n",
    "                    'Accuracy': np.mean(scores['test_accuracy']),\n",
    "                    'Precision': np.mean(scores['test_precision_macro']),\n",
    "                    'Recall': np.mean(scores['test_recall_macro']),\n",
    "                    'F1': np.mean(scores['test_f1_macro']),\n",
    "                    'Accuracy Std': np.std(scores['test_accuracy']),\n",
    "                    'Precision Std': np.std(scores['test_precision_macro']),\n",
    "                    'Recall Std': np.std(scores['test_recall_macro']),\n",
    "                    'F1 Std': np.std(scores['test_f1_macro'])\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: lam=10000000.0, p=0.005, window_size=51.0, poly_order=2.0 with Accuracy: 0.9370\n"
     ]
    }
   ],
   "source": [
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Identify the best parameters based on the selected criteria\n",
    "best_row = results_df.loc[results_df['Accuracy'].idxmax()]\n",
    "\n",
    "print(f\"Best Parameters: lam={best_row['lam']}, p={best_row['p']}, window_size={best_row['window']}, poly_order={best_row['poly']} with Accuracy: {best_row['Accuracy']:.4f}\")\n",
    "\n",
    "# Save the results to csv\n",
    "results_df.sort_values('Accuracy', ascending=False).to_csv(\"Scaled_Smoothed_Baseline_Results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
